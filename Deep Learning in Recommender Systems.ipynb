{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing movie posters using the VGG16 model in Keras\n",
    "\n",
    "In this notebook I use the VGG16 model as feature extractor. These features, characterizing each poster, are used to measure similarity at a group and a individual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Extracting features using the VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load meta data and posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd\n",
    "import os\n",
    "os.chdir(\"/Users/Mads/Documents/Poster Analysis/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full movie dataset\n",
    "movie_data = pd.read_csv(\"movie_dataset.csv\", encoding=\"ISO-8859-1\", usecols=[\"imdbId\", \"Title\", \"Genre\"])\n",
    "movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of poster paths for all posters in the folder containing downloaded posters\n",
    "import os\n",
    "\n",
    "# setting directory\n",
    "directory = \"/Users/Mads/Documents/AllPosters/\"\n",
    "\n",
    "# excludng .DS_Store which all mac os folders have\n",
    "poster_paths = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "if directory + '.DS_Store' in poster_paths:\n",
    "    poster_paths.remove(directory + '.DS_Store')\n",
    "    \n",
    "len(poster_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset for testing\n",
    "import random\n",
    "random.seed(9)\n",
    "n = 5000\n",
    "r_poster_p = random.sample(poster_paths, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of movie ids from the list of posters paths\n",
    "# is used to print posters later\n",
    "\n",
    "import re\n",
    "poster_ids_temp = []\n",
    "\n",
    "for i in poster_paths:\n",
    "    poster_ids_temp.append(re.findall('\\d+', i))\n",
    "\n",
    "# make flat list out of the above id list\n",
    "poster_ids = []\n",
    "\n",
    "for sublist in poster_ids_temp:\n",
    "    for item in sublist:\n",
    "        poster_ids.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster_ids.index('113277')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of movie ids from the list of posters paths\n",
    "# is used to print posters later\n",
    "\n",
    "import re\n",
    "sub_ids_temp = []\n",
    "\n",
    "for i in r_poster_p:\n",
    "    sub_ids_temp.append(re.findall('\\d+', i))\n",
    "\n",
    "# make flat list out of the above id list\n",
    "sub_ids = []\n",
    "\n",
    "for sublist in sub_ids_temp:\n",
    "    for item in sublist:\n",
    "        sub_ids.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_ids.index('113277')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all movie posters\n",
    "\n",
    "Poster images are preprocessed using the build-in functions of the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing each movie poster\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image as kimage\n",
    "\n",
    "pre_img_list = []\n",
    "n = 0\n",
    "\n",
    "import os\n",
    "for p in r_poster_p:\n",
    "    orig_img = kimage.load_img(p, target_size=(224, 224))\n",
    "    array_img = kimage.img_to_array(orig_img)\n",
    "    batch_img = np.expand_dims(array_img, axis=0)\n",
    "    pre_single = preprocess_input(batch_img)\n",
    "    pre_img_list.append(pre_single)\n",
    "    n = n+1\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a feature vector for all posters\n",
    "\n",
    "Note that the top layers of the model are not included. By default, the network has three fully connected layers and an output layer with 1000 categories. In this case, we are interested in using the model to extract features from the movie posters, leaving the pretrained weights unchanced. Therefore, we exclude the last layers and only use the features represented by the last max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "from keras.applications import VGG16\n",
    "model = VGG16(include_top = False, weights='imagenet')\n",
    "\n",
    "#model.summary()\n",
    "#model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a prediction list and the output matrix \n",
    "prediction = [0]*len(r_poster_p)\n",
    "feat_matrix = np.zeros([len(r_poster_p),25088])\n",
    "\n",
    "# get features for each poster\n",
    "for i in range(len(r_poster_p)):\n",
    "    prediction[i] = model.predict(pre_img_list[i]).ravel()\n",
    "    feat_matrix[i,:] = prediction[i] \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sparse matrix\n",
    "from scipy import sparse\n",
    "feat_csr = sparse.csr_matrix(feat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full sparse matrix to disk\n",
    "from scipy import sparse\n",
    "sparse.save_npz('feat_csr.npz', feat_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET convert to sparse matrix\n",
    "from scipy import sparse\n",
    "sub_feat_csr = sparse.csr_matrix(feat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET save sparse matrix to disk\n",
    "from scipy import sparse\n",
    "sparse.save_npz('sub_feat_csr.npz', sub_feat_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sparse matrix\n",
    "from scipy import sparse\n",
    "feat_csr = sparse.load_npz('feat_csr.npz')\n",
    "feat_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET load sparse matrix\n",
    "from scipy import sparse\n",
    "sub_feat_csr = sparse.load_npz('sub_feat_csr.npz')\n",
    "sub_feat_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sparse matrix back to dense\n",
    "from scipy import sparse\n",
    "sub_feat = sparse.csr_matrix.todense(sub_feat_csr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Clustering: measuring similarity at group level\n",
    "\n",
    "Similarities at a group level can now be measured using the k-means clustering  algorithm. Since this is a case of unsupervised clustering with no ground truth, I will be using the inertia value and the Silhoutte coefficient as a performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "norm = Normalizer()\n",
    "pipeline = make_pipeline(norm,kmeans)\n",
    "norm_clusters = pipeline.fit_predict(sub_feat_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting balance of clusters\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(norm_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "norm_feat = preprocessing.normalize(sub_feat)\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++').fit(norm_feat)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn import metrics\n",
    "labels = kmeans.labels_\n",
    "print(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with a range of k's and saving inertia and silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "ks = range(5,26)\n",
    "inertias = []\n",
    "sils = []\n",
    "norm = Normalizer()\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k)\n",
    "    pipeline = make_pipeline(norm,kmeans)\n",
    "    pipeline.fit(sub_feat)\n",
    "    inertia = pipeline.named_steps['kmeans'].inertia_\n",
    "    inertias.append(inertia)\n",
    "    labels = kmeans.labels_\n",
    "    sils.append(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save intertia list\n",
    "import numpy\n",
    "numpy.savetxt(\"file_inertias.csv\", inertias, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inertia list\n",
    "import numpy\n",
    "inertias = numpy.loadtxt(\"file_inertias.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sils list\n",
    "import numpy\n",
    "numpy.savetxt(\"file_sils.csv\", sils, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sils list\n",
    "import numpy\n",
    "sils = numpy.loadtxt(\"file_sils.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting inertia for a range of ks\n",
    "ks = range(5,26)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks,inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Silhouette Score for a range of ks\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks, sils, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Dimensionality reduction using PCA and NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "In an attempt to overcome the curse of dimensionality, PCA is implemented to reduce the dimensions of the data from 25088 to a smaller number of principal components.\n",
    "\n",
    "- This results in an increase in computational efficiency, however, the Silhouette score remains close to 0, indicating poor clustering.\n",
    "\n",
    "- Increasing the number of components until explained variance is near 100% does not improve clustering performance either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "norm = Normalizer()\n",
    "pca = PCA(n_components = 50)\n",
    "pipeline = make_pipeline(norm, pca)\n",
    "pca_comp = pipeline.fit_transform(sub_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++').fit(pca_comp)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn import metrics\n",
    "metrics.silhouette_score(sub_feat, labels)\n",
    "print(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explained variance\n",
    "pca_variance = pca.explained_variance_ratio_\n",
    "sum(pca_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of explained variance for the PCA components\n",
    "import matplotlib.pyplot as plt\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.xticks(features)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with a range of k's and saving inertia and silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "\n",
    "ks = range(5,26)\n",
    "inertias_pca = []\n",
    "sils_pca = []\n",
    "norm = Normalizer()\n",
    "pca = PCA(n_components = 50)\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k,init='k-means++')\n",
    "    pipeline = make_pipeline(norm,pca,kmeans)\n",
    "    pipeline.fit(sub_feat)\n",
    "    inertia = pipeline.named_steps['kmeans'].inertia_\n",
    "    inertias_pca.append(inertia)\n",
    "    labels = kmeans.labels_\n",
    "    sils_pca.append(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting inertia over a range of k\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks,inertias_pca, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sils_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting inertia over a range of k\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks, sils_pca, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF\n",
    "Since PCA did not improve clustering performance, NMF was implemented as a second attempt. Since the values of the matrix are all non-negative, this as approach which, compared to PCA, preserves interpretibility of the components. For instance, NMF represents images as combinations of common patterns (Wilson, 2018). Hence, even though the raw image features are abstract, the NMF components can be seen as topics or clusters. \n",
    "\n",
    "Result: NMF did not improve clustering performance either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "nmf = NMF(n_components = 50)\n",
    "norm = Normalizer()\n",
    "pipeline = make_pipeline(nmf, norm)\n",
    "nmf_feat = pipeline.fit_transform(sub_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init='k-means++').fit(nmf_feat)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "print(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans with a range of k's and saving inertia and silhouette score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "ks = range(5,26)\n",
    "inertias_nmf = []\n",
    "sils_nmf = []\n",
    "\n",
    "for k in ks:\n",
    "    kmeans = KMeans(n_clusters = k).fit(nmf_feat)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias_nmf.append(inertia)\n",
    "    labels = kmeans.labels_\n",
    "    sils_nmf.append(metrics.silhouette_score(sub_feat, labels, metric='euclidean'))\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting inertia over a range of k\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks,inertias_nmf, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sils_nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting inertia over a range of k\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ks, sils_nmf, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('Silhouette Coefficient')\n",
    "plt.xticks(ks)\n",
    "plt.show()\n",
    "plt.savefig('inertia.png')\n",
    "plt.rcParams[\"figure.figsize\"] = [12,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Poster to poster similarity\n",
    "\n",
    "### Cosine similarity matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate cosine similarity matrix\n",
    "import numpy as np\n",
    "def cosine_similarity(features):\n",
    "    sim = features.dot(features.T)\n",
    "    if not isinstance(sim, np.ndarray):\n",
    "        sim = sim.toarray()\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculte similarity matrix \n",
    "sim = cosine_similarity(feat_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sim matrix to disk\n",
    "import numpy as np\n",
    "np.save('sim_file',sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sim matrix\n",
    "import numpy as np\n",
    "sim = np.load('sim_file.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "import pandas as pd\n",
    "sim_df = pd.DataFrame(sim)\n",
    "sim_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a given movie using the imdb ID get the index in the ID list.\n",
    "poster_ids.index('848228')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display posters with highest cosine similarity\n",
    "poster = sim_df.loc[36709]\n",
    "n_sim = poster.nlargest(n = 6)\n",
    "n_sim_d = pd.DataFrame(n_sim)\n",
    "n_sim_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of posters paths for the n most similar posters\n",
    "n_sim_poster = []\n",
    "\n",
    "for i in n_sim_d.index:\n",
    "    n_sim_poster.append(poster_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print n most similar poster\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "for i in n_sim_poster:\n",
    "    img = mpimg.imread(i)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
